<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>atomic.h source code [codebrowser/include/qemu/atomic.h] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../data/jquery/jquery-ui.min.js"></script>
<script>var file = 'codebrowser/include/qemu/atomic.h'; var root_path = '../../..'; var data_path = '../../../../data';</script>
<script src='../../../../data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../..'>codebrowser</a>/<a href='..'>include</a>/<a href='./'>qemu</a>/<a href='atomic.h.html'>atomic.h</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/*</i></td></tr>
<tr><th id="2">2</th><td><i> * Simple interface for atomic operations.</i></td></tr>
<tr><th id="3">3</th><td><i> *</i></td></tr>
<tr><th id="4">4</th><td><i> * Copyright (C) 2013 Red Hat, Inc.</i></td></tr>
<tr><th id="5">5</th><td><i> *</i></td></tr>
<tr><th id="6">6</th><td><i> * Author: Paolo Bonzini &lt;pbonzini@redhat.com&gt;</i></td></tr>
<tr><th id="7">7</th><td><i> *</i></td></tr>
<tr><th id="8">8</th><td><i> * This work is licensed under the terms of the GNU GPL, version 2 or later.</i></td></tr>
<tr><th id="9">9</th><td><i> * See the COPYING file in the top-level directory.</i></td></tr>
<tr><th id="10">10</th><td><i> *</i></td></tr>
<tr><th id="11">11</th><td><i> * See docs/atomics.txt for discussion about the guarantees each</i></td></tr>
<tr><th id="12">12</th><td><i> * atomic primitive is meant to provide.</i></td></tr>
<tr><th id="13">13</th><td><i> */</i></td></tr>
<tr><th id="14">14</th><td></td></tr>
<tr><th id="15">15</th><td><u>#<span data-ppcond="15">ifndef</span> <span class="macro" data-ref="_M/QEMU_ATOMIC_H">QEMU_ATOMIC_H</span></u></td></tr>
<tr><th id="16">16</th><td><u>#define <dfn class="macro" id="_M/QEMU_ATOMIC_H" data-ref="_M/QEMU_ATOMIC_H">QEMU_ATOMIC_H</dfn></u></td></tr>
<tr><th id="17">17</th><td></td></tr>
<tr><th id="18">18</th><td><i>/* Compiler barrier */</i></td></tr>
<tr><th id="19">19</th><td><u>#define <dfn class="macro" id="_M/barrier" data-ref="_M/barrier">barrier</dfn>()   ({ asm volatile("" ::: "memory"); (void)0; })</u></td></tr>
<tr><th id="20">20</th><td></td></tr>
<tr><th id="21">21</th><td><i>/* The variable that receives the old value of an atomically-accessed</i></td></tr>
<tr><th id="22">22</th><td><i> * variable must be non-qualified, because atomic builtins return values</i></td></tr>
<tr><th id="23">23</th><td><i> * through a pointer-type argument as in __atomic_load(&amp;var, &amp;old, MODEL).</i></td></tr>
<tr><th id="24">24</th><td><i> *</i></td></tr>
<tr><th id="25">25</th><td><i> * This macro has to handle types smaller than int manually, because of</i></td></tr>
<tr><th id="26">26</th><td><i> * implicit promotion.  int and larger types, as well as pointers, can be</i></td></tr>
<tr><th id="27">27</th><td><i> * converted to a non-qualified type just by applying a binary operator.</i></td></tr>
<tr><th id="28">28</th><td><i> */</i></td></tr>
<tr><th id="29">29</th><td><u>#define <dfn class="macro" id="_M/typeof_strip_qual" data-ref="_M/typeof_strip_qual">typeof_strip_qual</dfn>(expr)                                                    \</u></td></tr>
<tr><th id="30">30</th><td><u>  typeof(                                                                          \</u></td></tr>
<tr><th id="31">31</th><td><u>    __builtin_choose_expr(                                                         \</u></td></tr>
<tr><th id="32">32</th><td><u>      __builtin_types_compatible_p(typeof(expr), bool) ||                          \</u></td></tr>
<tr><th id="33">33</th><td><u>        __builtin_types_compatible_p(typeof(expr), const bool) ||                  \</u></td></tr>
<tr><th id="34">34</th><td><u>        __builtin_types_compatible_p(typeof(expr), volatile bool) ||               \</u></td></tr>
<tr><th id="35">35</th><td><u>        __builtin_types_compatible_p(typeof(expr), const volatile bool),           \</u></td></tr>
<tr><th id="36">36</th><td><u>        (bool)1,                                                                   \</u></td></tr>
<tr><th id="37">37</th><td><u>    __builtin_choose_expr(                                                         \</u></td></tr>
<tr><th id="38">38</th><td><u>      __builtin_types_compatible_p(typeof(expr), signed char) ||                   \</u></td></tr>
<tr><th id="39">39</th><td><u>        __builtin_types_compatible_p(typeof(expr), const signed char) ||           \</u></td></tr>
<tr><th id="40">40</th><td><u>        __builtin_types_compatible_p(typeof(expr), volatile signed char) ||        \</u></td></tr>
<tr><th id="41">41</th><td><u>        __builtin_types_compatible_p(typeof(expr), const volatile signed char),    \</u></td></tr>
<tr><th id="42">42</th><td><u>        (signed char)1,                                                            \</u></td></tr>
<tr><th id="43">43</th><td><u>    __builtin_choose_expr(                                                         \</u></td></tr>
<tr><th id="44">44</th><td><u>      __builtin_types_compatible_p(typeof(expr), unsigned char) ||                 \</u></td></tr>
<tr><th id="45">45</th><td><u>        __builtin_types_compatible_p(typeof(expr), const unsigned char) ||         \</u></td></tr>
<tr><th id="46">46</th><td><u>        __builtin_types_compatible_p(typeof(expr), volatile unsigned char) ||      \</u></td></tr>
<tr><th id="47">47</th><td><u>        __builtin_types_compatible_p(typeof(expr), const volatile unsigned char),  \</u></td></tr>
<tr><th id="48">48</th><td><u>        (unsigned char)1,                                                          \</u></td></tr>
<tr><th id="49">49</th><td><u>    __builtin_choose_expr(                                                         \</u></td></tr>
<tr><th id="50">50</th><td><u>      __builtin_types_compatible_p(typeof(expr), signed short) ||                  \</u></td></tr>
<tr><th id="51">51</th><td><u>        __builtin_types_compatible_p(typeof(expr), const signed short) ||          \</u></td></tr>
<tr><th id="52">52</th><td><u>        __builtin_types_compatible_p(typeof(expr), volatile signed short) ||       \</u></td></tr>
<tr><th id="53">53</th><td><u>        __builtin_types_compatible_p(typeof(expr), const volatile signed short),   \</u></td></tr>
<tr><th id="54">54</th><td><u>        (signed short)1,                                                           \</u></td></tr>
<tr><th id="55">55</th><td><u>    __builtin_choose_expr(                                                         \</u></td></tr>
<tr><th id="56">56</th><td><u>      __builtin_types_compatible_p(typeof(expr), unsigned short) ||                \</u></td></tr>
<tr><th id="57">57</th><td><u>        __builtin_types_compatible_p(typeof(expr), const unsigned short) ||        \</u></td></tr>
<tr><th id="58">58</th><td><u>        __builtin_types_compatible_p(typeof(expr), volatile unsigned short) ||     \</u></td></tr>
<tr><th id="59">59</th><td><u>        __builtin_types_compatible_p(typeof(expr), const volatile unsigned short), \</u></td></tr>
<tr><th id="60">60</th><td><u>        (unsigned short)1,                                                         \</u></td></tr>
<tr><th id="61">61</th><td><u>      (expr)+0))))))</u></td></tr>
<tr><th id="62">62</th><td></td></tr>
<tr><th id="63">63</th><td><u>#<span data-ppcond="63">ifdef</span> <span class="macro" data-ref="_M/__ATOMIC_RELAXED">__ATOMIC_RELAXED</span></u></td></tr>
<tr><th id="64">64</th><td><i>/* For C11 atomic ops */</i></td></tr>
<tr><th id="65">65</th><td></td></tr>
<tr><th id="66">66</th><td><i>/* Manual memory barriers</i></td></tr>
<tr><th id="67">67</th><td><i> *</i></td></tr>
<tr><th id="68">68</th><td><i> *__atomic_thread_fence does not include a compiler barrier; instead,</i></td></tr>
<tr><th id="69">69</th><td><i> * the barrier is part of __atomic_load/__atomic_store's "volatile-like"</i></td></tr>
<tr><th id="70">70</th><td><i> * semantics. If smp_wmb() is a no-op, absence of the barrier means that</i></td></tr>
<tr><th id="71">71</th><td><i> * the compiler is free to reorder stores on each side of the barrier.</i></td></tr>
<tr><th id="72">72</th><td><i> * Add one here, and similarly in smp_rmb() and smp_read_barrier_depends().</i></td></tr>
<tr><th id="73">73</th><td><i> */</i></td></tr>
<tr><th id="74">74</th><td></td></tr>
<tr><th id="75">75</th><td><u>#define <dfn class="macro" id="_M/smp_mb" data-ref="_M/smp_mb">smp_mb</dfn>()                     ({ barrier(); __atomic_thread_fence(__ATOMIC_SEQ_CST); })</u></td></tr>
<tr><th id="76">76</th><td><u>#define <dfn class="macro" id="_M/smp_mb_release" data-ref="_M/smp_mb_release">smp_mb_release</dfn>()             ({ barrier(); __atomic_thread_fence(__ATOMIC_RELEASE); })</u></td></tr>
<tr><th id="77">77</th><td><u>#define <dfn class="macro" id="_M/smp_mb_acquire" data-ref="_M/smp_mb_acquire">smp_mb_acquire</dfn>()             ({ barrier(); __atomic_thread_fence(__ATOMIC_ACQUIRE); })</u></td></tr>
<tr><th id="78">78</th><td></td></tr>
<tr><th id="79">79</th><td><i>/* Most compilers currently treat consume and acquire the same, but really</i></td></tr>
<tr><th id="80">80</th><td><i> * no processors except Alpha need a barrier here.  Leave it in if</i></td></tr>
<tr><th id="81">81</th><td><i> * using Thread Sanitizer to avoid warnings, otherwise optimize it away.</i></td></tr>
<tr><th id="82">82</th><td><i> */</i></td></tr>
<tr><th id="83">83</th><td><u>#<span data-ppcond="83">if</span> defined(<span class="macro" data-ref="_M/__SANITIZE_THREAD__">__SANITIZE_THREAD__</span>)</u></td></tr>
<tr><th id="84">84</th><td><u>#define smp_read_barrier_depends()   ({ barrier(); __atomic_thread_fence(__ATOMIC_CONSUME); })</u></td></tr>
<tr><th id="85">85</th><td><u>#<span data-ppcond="83">elif</span> defined(<span class="macro" data-ref="_M/__alpha__">__alpha__</span>)</u></td></tr>
<tr><th id="86">86</th><td><u>#define smp_read_barrier_depends()   asm volatile("mb":::"memory")</u></td></tr>
<tr><th id="87">87</th><td><u>#<span data-ppcond="83">else</span></u></td></tr>
<tr><th id="88">88</th><td><u>#define <dfn class="macro" id="_M/smp_read_barrier_depends" data-ref="_M/smp_read_barrier_depends">smp_read_barrier_depends</dfn>()   barrier()</u></td></tr>
<tr><th id="89">89</th><td><u>#<span data-ppcond="83">endif</span></u></td></tr>
<tr><th id="90">90</th><td></td></tr>
<tr><th id="91">91</th><td><i>/* Sanity check that the size of an atomic operation isn't "overly large".</i></td></tr>
<tr><th id="92">92</th><td><i> * Despite the fact that e.g. i686 has 64-bit atomic operations, we do not</i></td></tr>
<tr><th id="93">93</th><td><i> * want to use them because we ought not need them, and this lets us do a</i></td></tr>
<tr><th id="94">94</th><td><i> * bit of sanity checking that other 32-bit hosts might build.</i></td></tr>
<tr><th id="95">95</th><td><i> *</i></td></tr>
<tr><th id="96">96</th><td><i> * That said, we have a problem on 64-bit ILP32 hosts in that in order to</i></td></tr>
<tr><th id="97">97</th><td><i> * sync with TCG_OVERSIZED_GUEST, this must match TCG_TARGET_REG_BITS.</i></td></tr>
<tr><th id="98">98</th><td><i> * We'd prefer not want to pull in everything else TCG related, so handle</i></td></tr>
<tr><th id="99">99</th><td><i> * those few cases by hand.</i></td></tr>
<tr><th id="100">100</th><td><i> *</i></td></tr>
<tr><th id="101">101</th><td><i> * Note that x32 is fully detected with __x64_64__ + _ILP32, and that for</i></td></tr>
<tr><th id="102">102</th><td><i> * Sparc we always force the use of sparcv9 in configure.</i></td></tr>
<tr><th id="103">103</th><td><i> */</i></td></tr>
<tr><th id="104">104</th><td><u>#<span data-ppcond="104">if</span> defined(<span class="macro" data-ref="_M/__x86_64__">__x86_64__</span>) || defined(<span class="macro" data-ref="_M/__sparc__">__sparc__</span>)</u></td></tr>
<tr><th id="105">105</th><td><u># define <dfn class="macro" id="_M/ATOMIC_REG_SIZE" data-ref="_M/ATOMIC_REG_SIZE">ATOMIC_REG_SIZE</dfn>  8</u></td></tr>
<tr><th id="106">106</th><td><u>#<span data-ppcond="104">else</span></u></td></tr>
<tr><th id="107">107</th><td><u># define ATOMIC_REG_SIZE  sizeof(void *)</u></td></tr>
<tr><th id="108">108</th><td><u>#<span data-ppcond="104">endif</span></u></td></tr>
<tr><th id="109">109</th><td></td></tr>
<tr><th id="110">110</th><td><i>/* Weak atomic operations prevent the compiler moving other</i></td></tr>
<tr><th id="111">111</th><td><i> * loads/stores past the atomic operation load/store. However there is</i></td></tr>
<tr><th id="112">112</th><td><i> * no explicit memory barrier for the processor.</i></td></tr>
<tr><th id="113">113</th><td><i> *</i></td></tr>
<tr><th id="114">114</th><td><i> * The C11 memory model says that variables that are accessed from</i></td></tr>
<tr><th id="115">115</th><td><i> * different threads should at least be done with __ATOMIC_RELAXED</i></td></tr>
<tr><th id="116">116</th><td><i> * primitives or the result is undefined. Generally this has little to</i></td></tr>
<tr><th id="117">117</th><td><i> * no effect on the generated code but not using the atomic primitives</i></td></tr>
<tr><th id="118">118</th><td><i> * will get flagged by sanitizers as a violation.</i></td></tr>
<tr><th id="119">119</th><td><i> */</i></td></tr>
<tr><th id="120">120</th><td><u>#define <dfn class="macro" id="_M/atomic_read__nocheck" data-ref="_M/atomic_read__nocheck">atomic_read__nocheck</dfn>(ptr) \</u></td></tr>
<tr><th id="121">121</th><td><u>    __atomic_load_n(ptr, __ATOMIC_RELAXED)</u></td></tr>
<tr><th id="122">122</th><td></td></tr>
<tr><th id="123">123</th><td><u>#define <dfn class="macro" id="_M/atomic_read" data-ref="_M/atomic_read">atomic_read</dfn>(ptr)                              \</u></td></tr>
<tr><th id="124">124</th><td><u>    ({                                                \</u></td></tr>
<tr><th id="125">125</th><td><u>    QEMU_BUILD_BUG_ON(sizeof(*ptr) &gt; ATOMIC_REG_SIZE); \</u></td></tr>
<tr><th id="126">126</th><td><u>    atomic_read__nocheck(ptr);                        \</u></td></tr>
<tr><th id="127">127</th><td><u>    })</u></td></tr>
<tr><th id="128">128</th><td></td></tr>
<tr><th id="129">129</th><td><u>#define <dfn class="macro" id="_M/atomic_set__nocheck" data-ref="_M/atomic_set__nocheck">atomic_set__nocheck</dfn>(ptr, i) \</u></td></tr>
<tr><th id="130">130</th><td><u>    __atomic_store_n(ptr, i, __ATOMIC_RELAXED)</u></td></tr>
<tr><th id="131">131</th><td></td></tr>
<tr><th id="132">132</th><td><u>#define <dfn class="macro" id="_M/atomic_set" data-ref="_M/atomic_set">atomic_set</dfn>(ptr, i)  do {                      \</u></td></tr>
<tr><th id="133">133</th><td><u>    QEMU_BUILD_BUG_ON(sizeof(*ptr) &gt; ATOMIC_REG_SIZE); \</u></td></tr>
<tr><th id="134">134</th><td><u>    atomic_set__nocheck(ptr, i);                      \</u></td></tr>
<tr><th id="135">135</th><td><u>} while(0)</u></td></tr>
<tr><th id="136">136</th><td></td></tr>
<tr><th id="137">137</th><td><i>/* See above: most compilers currently treat consume and acquire the</i></td></tr>
<tr><th id="138">138</th><td><i> * same, but this slows down atomic_rcu_read unnecessarily.</i></td></tr>
<tr><th id="139">139</th><td><i> */</i></td></tr>
<tr><th id="140">140</th><td><u>#<span data-ppcond="140">ifdef</span> <span class="macro" data-ref="_M/__SANITIZE_THREAD__">__SANITIZE_THREAD__</span></u></td></tr>
<tr><th id="141">141</th><td><u>#define atomic_rcu_read__nocheck(ptr, valptr)           \</u></td></tr>
<tr><th id="142">142</th><td><u>    __atomic_load(ptr, valptr, __ATOMIC_CONSUME);</u></td></tr>
<tr><th id="143">143</th><td><u>#<span data-ppcond="140">else</span></u></td></tr>
<tr><th id="144">144</th><td><u>#define <dfn class="macro" id="_M/atomic_rcu_read__nocheck" data-ref="_M/atomic_rcu_read__nocheck">atomic_rcu_read__nocheck</dfn>(ptr, valptr)           \</u></td></tr>
<tr><th id="145">145</th><td><u>    __atomic_load(ptr, valptr, __ATOMIC_RELAXED);       \</u></td></tr>
<tr><th id="146">146</th><td><u>    smp_read_barrier_depends();</u></td></tr>
<tr><th id="147">147</th><td><u>#<span data-ppcond="140">endif</span></u></td></tr>
<tr><th id="148">148</th><td></td></tr>
<tr><th id="149">149</th><td><u>#define <dfn class="macro" id="_M/atomic_rcu_read" data-ref="_M/atomic_rcu_read">atomic_rcu_read</dfn>(ptr)                          \</u></td></tr>
<tr><th id="150">150</th><td><u>    ({                                                \</u></td></tr>
<tr><th id="151">151</th><td><u>    QEMU_BUILD_BUG_ON(sizeof(*ptr) &gt; ATOMIC_REG_SIZE); \</u></td></tr>
<tr><th id="152">152</th><td><u>    typeof_strip_qual(*ptr) _val;                     \</u></td></tr>
<tr><th id="153">153</th><td><u>    atomic_rcu_read__nocheck(ptr, &amp;_val);             \</u></td></tr>
<tr><th id="154">154</th><td><u>    _val;                                             \</u></td></tr>
<tr><th id="155">155</th><td><u>    })</u></td></tr>
<tr><th id="156">156</th><td></td></tr>
<tr><th id="157">157</th><td><u>#define <dfn class="macro" id="_M/atomic_rcu_set" data-ref="_M/atomic_rcu_set">atomic_rcu_set</dfn>(ptr, i) do {                   \</u></td></tr>
<tr><th id="158">158</th><td><u>    QEMU_BUILD_BUG_ON(sizeof(*ptr) &gt; ATOMIC_REG_SIZE); \</u></td></tr>
<tr><th id="159">159</th><td><u>    __atomic_store_n(ptr, i, __ATOMIC_RELEASE);       \</u></td></tr>
<tr><th id="160">160</th><td><u>} while(0)</u></td></tr>
<tr><th id="161">161</th><td></td></tr>
<tr><th id="162">162</th><td><u>#define <dfn class="macro" id="_M/atomic_load_acquire" data-ref="_M/atomic_load_acquire">atomic_load_acquire</dfn>(ptr)                        \</u></td></tr>
<tr><th id="163">163</th><td><u>    ({                                                  \</u></td></tr>
<tr><th id="164">164</th><td><u>    QEMU_BUILD_BUG_ON(sizeof(*ptr) &gt; ATOMIC_REG_SIZE);  \</u></td></tr>
<tr><th id="165">165</th><td><u>    typeof_strip_qual(*ptr) _val;                       \</u></td></tr>
<tr><th id="166">166</th><td><u>    __atomic_load(ptr, &amp;_val, __ATOMIC_ACQUIRE);        \</u></td></tr>
<tr><th id="167">167</th><td><u>    _val;                                               \</u></td></tr>
<tr><th id="168">168</th><td><u>    })</u></td></tr>
<tr><th id="169">169</th><td></td></tr>
<tr><th id="170">170</th><td><u>#define <dfn class="macro" id="_M/atomic_store_release" data-ref="_M/atomic_store_release">atomic_store_release</dfn>(ptr, i)  do {              \</u></td></tr>
<tr><th id="171">171</th><td><u>    QEMU_BUILD_BUG_ON(sizeof(*ptr) &gt; ATOMIC_REG_SIZE);  \</u></td></tr>
<tr><th id="172">172</th><td><u>    __atomic_store_n(ptr, i, __ATOMIC_RELEASE);         \</u></td></tr>
<tr><th id="173">173</th><td><u>} while(0)</u></td></tr>
<tr><th id="174">174</th><td></td></tr>
<tr><th id="175">175</th><td></td></tr>
<tr><th id="176">176</th><td><i>/* All the remaining operations are fully sequentially consistent */</i></td></tr>
<tr><th id="177">177</th><td></td></tr>
<tr><th id="178">178</th><td><u>#define <dfn class="macro" id="_M/atomic_xchg__nocheck" data-ref="_M/atomic_xchg__nocheck">atomic_xchg__nocheck</dfn>(ptr, i)    ({                  \</u></td></tr>
<tr><th id="179">179</th><td><u>    __atomic_exchange_n(ptr, (i), __ATOMIC_SEQ_CST);        \</u></td></tr>
<tr><th id="180">180</th><td><u>})</u></td></tr>
<tr><th id="181">181</th><td></td></tr>
<tr><th id="182">182</th><td><u>#define <dfn class="macro" id="_M/atomic_xchg" data-ref="_M/atomic_xchg">atomic_xchg</dfn>(ptr, i)    ({                           \</u></td></tr>
<tr><th id="183">183</th><td><u>    QEMU_BUILD_BUG_ON(sizeof(*ptr) &gt; ATOMIC_REG_SIZE);      \</u></td></tr>
<tr><th id="184">184</th><td><u>    atomic_xchg__nocheck(ptr, i);                           \</u></td></tr>
<tr><th id="185">185</th><td><u>})</u></td></tr>
<tr><th id="186">186</th><td></td></tr>
<tr><th id="187">187</th><td><i>/* Returns the eventual value, failed or not */</i></td></tr>
<tr><th id="188">188</th><td><u>#define <dfn class="macro" id="_M/atomic_cmpxchg__nocheck" data-ref="_M/atomic_cmpxchg__nocheck">atomic_cmpxchg__nocheck</dfn>(ptr, old, new)    ({                    \</u></td></tr>
<tr><th id="189">189</th><td><u>    typeof_strip_qual(*ptr) _old = (old);                               \</u></td></tr>
<tr><th id="190">190</th><td><u>    __atomic_compare_exchange_n(ptr, &amp;_old, new, false,                 \</u></td></tr>
<tr><th id="191">191</th><td><u>                              __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);      \</u></td></tr>
<tr><th id="192">192</th><td><u>    _old;                                                               \</u></td></tr>
<tr><th id="193">193</th><td><u>})</u></td></tr>
<tr><th id="194">194</th><td></td></tr>
<tr><th id="195">195</th><td><u>#define <dfn class="macro" id="_M/atomic_cmpxchg" data-ref="_M/atomic_cmpxchg">atomic_cmpxchg</dfn>(ptr, old, new)    ({                             \</u></td></tr>
<tr><th id="196">196</th><td><u>    QEMU_BUILD_BUG_ON(sizeof(*ptr) &gt; ATOMIC_REG_SIZE);                  \</u></td></tr>
<tr><th id="197">197</th><td><u>    atomic_cmpxchg__nocheck(ptr, old, new);                             \</u></td></tr>
<tr><th id="198">198</th><td><u>})</u></td></tr>
<tr><th id="199">199</th><td></td></tr>
<tr><th id="200">200</th><td><i>/* Provide shorter names for GCC atomic builtins, return old value */</i></td></tr>
<tr><th id="201">201</th><td><u>#define <dfn class="macro" id="_M/atomic_fetch_inc" data-ref="_M/atomic_fetch_inc">atomic_fetch_inc</dfn>(ptr)  __atomic_fetch_add(ptr, 1, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="202">202</th><td><u>#define <dfn class="macro" id="_M/atomic_fetch_dec" data-ref="_M/atomic_fetch_dec">atomic_fetch_dec</dfn>(ptr)  __atomic_fetch_sub(ptr, 1, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="203">203</th><td><u>#define <dfn class="macro" id="_M/atomic_fetch_add" data-ref="_M/atomic_fetch_add">atomic_fetch_add</dfn>(ptr, n) __atomic_fetch_add(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="204">204</th><td><u>#define <dfn class="macro" id="_M/atomic_fetch_sub" data-ref="_M/atomic_fetch_sub">atomic_fetch_sub</dfn>(ptr, n) __atomic_fetch_sub(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="205">205</th><td><u>#define <dfn class="macro" id="_M/atomic_fetch_and" data-ref="_M/atomic_fetch_and">atomic_fetch_and</dfn>(ptr, n) __atomic_fetch_and(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="206">206</th><td><u>#define <dfn class="macro" id="_M/atomic_fetch_or" data-ref="_M/atomic_fetch_or">atomic_fetch_or</dfn>(ptr, n)  __atomic_fetch_or(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="207">207</th><td><u>#define <dfn class="macro" id="_M/atomic_fetch_xor" data-ref="_M/atomic_fetch_xor">atomic_fetch_xor</dfn>(ptr, n) __atomic_fetch_xor(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="208">208</th><td></td></tr>
<tr><th id="209">209</th><td><u>#define <dfn class="macro" id="_M/atomic_inc_fetch" data-ref="_M/atomic_inc_fetch">atomic_inc_fetch</dfn>(ptr)    __atomic_add_fetch(ptr, 1, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="210">210</th><td><u>#define <dfn class="macro" id="_M/atomic_dec_fetch" data-ref="_M/atomic_dec_fetch">atomic_dec_fetch</dfn>(ptr)    __atomic_sub_fetch(ptr, 1, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="211">211</th><td><u>#define <dfn class="macro" id="_M/atomic_add_fetch" data-ref="_M/atomic_add_fetch">atomic_add_fetch</dfn>(ptr, n) __atomic_add_fetch(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="212">212</th><td><u>#define <dfn class="macro" id="_M/atomic_sub_fetch" data-ref="_M/atomic_sub_fetch">atomic_sub_fetch</dfn>(ptr, n) __atomic_sub_fetch(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="213">213</th><td><u>#define <dfn class="macro" id="_M/atomic_and_fetch" data-ref="_M/atomic_and_fetch">atomic_and_fetch</dfn>(ptr, n) __atomic_and_fetch(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="214">214</th><td><u>#define <dfn class="macro" id="_M/atomic_or_fetch" data-ref="_M/atomic_or_fetch">atomic_or_fetch</dfn>(ptr, n)  __atomic_or_fetch(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="215">215</th><td><u>#define <dfn class="macro" id="_M/atomic_xor_fetch" data-ref="_M/atomic_xor_fetch">atomic_xor_fetch</dfn>(ptr, n) __atomic_xor_fetch(ptr, n, __ATOMIC_SEQ_CST)</u></td></tr>
<tr><th id="216">216</th><td></td></tr>
<tr><th id="217">217</th><td><i>/* And even shorter names that return void.  */</i></td></tr>
<tr><th id="218">218</th><td><u>#define <dfn class="macro" id="_M/atomic_inc" data-ref="_M/atomic_inc">atomic_inc</dfn>(ptr)    ((void) __atomic_fetch_add(ptr, 1, __ATOMIC_SEQ_CST))</u></td></tr>
<tr><th id="219">219</th><td><u>#define <dfn class="macro" id="_M/atomic_dec" data-ref="_M/atomic_dec">atomic_dec</dfn>(ptr)    ((void) __atomic_fetch_sub(ptr, 1, __ATOMIC_SEQ_CST))</u></td></tr>
<tr><th id="220">220</th><td><u>#define <dfn class="macro" id="_M/atomic_add" data-ref="_M/atomic_add">atomic_add</dfn>(ptr, n) ((void) __atomic_fetch_add(ptr, n, __ATOMIC_SEQ_CST))</u></td></tr>
<tr><th id="221">221</th><td><u>#define <dfn class="macro" id="_M/atomic_sub" data-ref="_M/atomic_sub">atomic_sub</dfn>(ptr, n) ((void) __atomic_fetch_sub(ptr, n, __ATOMIC_SEQ_CST))</u></td></tr>
<tr><th id="222">222</th><td><u>#define <dfn class="macro" id="_M/atomic_and" data-ref="_M/atomic_and">atomic_and</dfn>(ptr, n) ((void) __atomic_fetch_and(ptr, n, __ATOMIC_SEQ_CST))</u></td></tr>
<tr><th id="223">223</th><td><u>#define <dfn class="macro" id="_M/atomic_or" data-ref="_M/atomic_or">atomic_or</dfn>(ptr, n)  ((void) __atomic_fetch_or(ptr, n, __ATOMIC_SEQ_CST))</u></td></tr>
<tr><th id="224">224</th><td><u>#define <dfn class="macro" id="_M/atomic_xor" data-ref="_M/atomic_xor">atomic_xor</dfn>(ptr, n) ((void) __atomic_fetch_xor(ptr, n, __ATOMIC_SEQ_CST))</u></td></tr>
<tr><th id="225">225</th><td></td></tr>
<tr><th id="226">226</th><td><u>#<span data-ppcond="63">else</span> /* __ATOMIC_RELAXED */</u></td></tr>
<tr><th id="227">227</th><td></td></tr>
<tr><th id="228">228</th><td><i>/*</i></td></tr>
<tr><th id="229">229</th><td><i> * We use GCC builtin if it's available, as that can use mfence on</i></td></tr>
<tr><th id="230">230</th><td><i> * 32-bit as well, e.g. if built with -march=pentium-m. However, on</i></td></tr>
<tr><th id="231">231</th><td><i> * i386 the spec is buggy, and the implementation followed it until</i></td></tr>
<tr><th id="232">232</th><td><i> * 4.3 (<a href="http://gcc.gnu.org/bugzilla/show_bug.cgi?id=36793">http://gcc.gnu.org/bugzilla/show_bug.cgi?id=36793</a>).</i></td></tr>
<tr><th id="233">233</th><td><i> */</i></td></tr>
<tr><th id="234">234</th><td><u>#if defined(__i386__) || defined(__x86_64__)</u></td></tr>
<tr><th id="235">235</th><td><u>#if !QEMU_GNUC_PREREQ(4, 4)</u></td></tr>
<tr><th id="236">236</th><td><u>#if defined __x86_64__</u></td></tr>
<tr><th id="237">237</th><td><u>#define smp_mb()    ({ asm volatile("mfence" ::: "memory"); (void)0; })</u></td></tr>
<tr><th id="238">238</th><td><u>#else</u></td></tr>
<tr><th id="239">239</th><td><u>#define smp_mb()    ({ asm volatile("lock; addl $0,0(%%esp) " ::: "memory"); (void)0; })</u></td></tr>
<tr><th id="240">240</th><td><u>#endif</u></td></tr>
<tr><th id="241">241</th><td><u>#endif</u></td></tr>
<tr><th id="242">242</th><td><u>#endif</u></td></tr>
<tr><th id="243">243</th><td></td></tr>
<tr><th id="244">244</th><td></td></tr>
<tr><th id="245">245</th><td><u>#ifdef __alpha__</u></td></tr>
<tr><th id="246">246</th><td><u>#define smp_read_barrier_depends()   asm volatile("mb":::"memory")</u></td></tr>
<tr><th id="247">247</th><td><u>#endif</u></td></tr>
<tr><th id="248">248</th><td></td></tr>
<tr><th id="249">249</th><td><u>#if defined(__i386__) || defined(__x86_64__) || defined(__s390x__)</u></td></tr>
<tr><th id="250">250</th><td></td></tr>
<tr><th id="251">251</th><td><i>/*</i></td></tr>
<tr><th id="252">252</th><td><i> * Because of the strongly ordered storage model, wmb() and rmb() are nops</i></td></tr>
<tr><th id="253">253</th><td><i> * here (a compiler barrier only).  QEMU doesn't do accesses to write-combining</i></td></tr>
<tr><th id="254">254</th><td><i> * qemu memory or non-temporal load/stores from C code.</i></td></tr>
<tr><th id="255">255</th><td><i> */</i></td></tr>
<tr><th id="256">256</th><td><u>#define smp_mb_release()   barrier()</u></td></tr>
<tr><th id="257">257</th><td><u>#define smp_mb_acquire()   barrier()</u></td></tr>
<tr><th id="258">258</th><td></td></tr>
<tr><th id="259">259</th><td><i>/*</i></td></tr>
<tr><th id="260">260</th><td><i> * __sync_lock_test_and_set() is documented to be an acquire barrier only,</i></td></tr>
<tr><th id="261">261</th><td><i> * but it is a full barrier at the hardware level.  Add a compiler barrier</i></td></tr>
<tr><th id="262">262</th><td><i> * to make it a full barrier also at the compiler level.</i></td></tr>
<tr><th id="263">263</th><td><i> */</i></td></tr>
<tr><th id="264">264</th><td><u>#define atomic_xchg(ptr, i)    (barrier(), __sync_lock_test_and_set(ptr, i))</u></td></tr>
<tr><th id="265">265</th><td></td></tr>
<tr><th id="266">266</th><td><u>#elif defined(_ARCH_PPC)</u></td></tr>
<tr><th id="267">267</th><td></td></tr>
<tr><th id="268">268</th><td><i>/*</i></td></tr>
<tr><th id="269">269</th><td><i> * We use an eieio() for wmb() on powerpc.  This assumes we don't</i></td></tr>
<tr><th id="270">270</th><td><i> * need to order cacheable and non-cacheable stores with respect to</i></td></tr>
<tr><th id="271">271</th><td><i> * each other.</i></td></tr>
<tr><th id="272">272</th><td><i> *</i></td></tr>
<tr><th id="273">273</th><td><i> * smp_mb has the same problem as on x86 for not-very-new GCC</i></td></tr>
<tr><th id="274">274</th><td><i> * (<a href="http://patchwork.ozlabs.org/patch/126184/,">http://patchwork.ozlabs.org/patch/126184/,</a> Nov 2011).</i></td></tr>
<tr><th id="275">275</th><td><i> */</i></td></tr>
<tr><th id="276">276</th><td><u>#define smp_wmb()          ({ asm volatile("eieio" ::: "memory"); (void)0; })</u></td></tr>
<tr><th id="277">277</th><td><u>#if defined(__powerpc64__)</u></td></tr>
<tr><th id="278">278</th><td><u>#define smp_mb_release()   ({ asm volatile("lwsync" ::: "memory"); (void)0; })</u></td></tr>
<tr><th id="279">279</th><td><u>#define smp_mb_acquire()   ({ asm volatile("lwsync" ::: "memory"); (void)0; })</u></td></tr>
<tr><th id="280">280</th><td><u>#else</u></td></tr>
<tr><th id="281">281</th><td><u>#define smp_mb_release()   ({ asm volatile("sync" ::: "memory"); (void)0; })</u></td></tr>
<tr><th id="282">282</th><td><u>#define smp_mb_acquire()   ({ asm volatile("sync" ::: "memory"); (void)0; })</u></td></tr>
<tr><th id="283">283</th><td><u>#endif</u></td></tr>
<tr><th id="284">284</th><td><u>#define smp_mb()           ({ asm volatile("sync" ::: "memory"); (void)0; })</u></td></tr>
<tr><th id="285">285</th><td></td></tr>
<tr><th id="286">286</th><td><u>#endif /* _ARCH_PPC */</u></td></tr>
<tr><th id="287">287</th><td></td></tr>
<tr><th id="288">288</th><td><i>/*</i></td></tr>
<tr><th id="289">289</th><td><i> * For (host) platforms we don't have explicit barrier definitions</i></td></tr>
<tr><th id="290">290</th><td><i> * for, we use the gcc __sync_synchronize() primitive to generate a</i></td></tr>
<tr><th id="291">291</th><td><i> * full barrier.  This should be safe on all platforms, though it may</i></td></tr>
<tr><th id="292">292</th><td><i> * be overkill for smp_mb_acquire() and smp_mb_release().</i></td></tr>
<tr><th id="293">293</th><td><i> */</i></td></tr>
<tr><th id="294">294</th><td><u>#ifndef smp_mb</u></td></tr>
<tr><th id="295">295</th><td><u>#define smp_mb()           __sync_synchronize()</u></td></tr>
<tr><th id="296">296</th><td><u>#endif</u></td></tr>
<tr><th id="297">297</th><td></td></tr>
<tr><th id="298">298</th><td><u>#ifndef smp_mb_acquire</u></td></tr>
<tr><th id="299">299</th><td><u>#define smp_mb_acquire()   __sync_synchronize()</u></td></tr>
<tr><th id="300">300</th><td><u>#endif</u></td></tr>
<tr><th id="301">301</th><td></td></tr>
<tr><th id="302">302</th><td><u>#ifndef smp_mb_release</u></td></tr>
<tr><th id="303">303</th><td><u>#define smp_mb_release()   __sync_synchronize()</u></td></tr>
<tr><th id="304">304</th><td><u>#endif</u></td></tr>
<tr><th id="305">305</th><td></td></tr>
<tr><th id="306">306</th><td><u>#ifndef smp_read_barrier_depends</u></td></tr>
<tr><th id="307">307</th><td><u>#define smp_read_barrier_depends()   barrier()</u></td></tr>
<tr><th id="308">308</th><td><u>#endif</u></td></tr>
<tr><th id="309">309</th><td></td></tr>
<tr><th id="310">310</th><td><i>/* These will only be atomic if the processor does the fetch or store</i></td></tr>
<tr><th id="311">311</th><td><i> * in a single issue memory operation</i></td></tr>
<tr><th id="312">312</th><td><i> */</i></td></tr>
<tr><th id="313">313</th><td><u>#define atomic_read__nocheck(p)   (*(__typeof__(*(p)) volatile*) (p))</u></td></tr>
<tr><th id="314">314</th><td><u>#define atomic_set__nocheck(p, i) ((*(__typeof__(*(p)) volatile*) (p)) = (i))</u></td></tr>
<tr><th id="315">315</th><td></td></tr>
<tr><th id="316">316</th><td><u>#define atomic_read(ptr)       atomic_read__nocheck(ptr)</u></td></tr>
<tr><th id="317">317</th><td><u>#define atomic_set(ptr, i)     atomic_set__nocheck(ptr,i)</u></td></tr>
<tr><th id="318">318</th><td></td></tr>
<tr><th id="319">319</th><td><i class="doc">/**</i></td></tr>
<tr><th id="320">320</th><td><i class="doc"> * atomic_rcu_read - reads a RCU-protected pointer to a local variable</i></td></tr>
<tr><th id="321">321</th><td><i class="doc"> * into a RCU read-side critical section. The pointer can later be safely</i></td></tr>
<tr><th id="322">322</th><td><i class="doc"> * dereferenced within the critical section.</i></td></tr>
<tr><th id="323">323</th><td><i class="doc"> *</i></td></tr>
<tr><th id="324">324</th><td><i class="doc"> * This ensures that the pointer copy is invariant thorough the whole critical</i></td></tr>
<tr><th id="325">325</th><td><i class="doc"> * section.</i></td></tr>
<tr><th id="326">326</th><td><i class="doc"> *</i></td></tr>
<tr><th id="327">327</th><td><i class="doc"> * Inserts memory barriers on architectures that require them (currently only</i></td></tr>
<tr><th id="328">328</th><td><i class="doc"> * Alpha) and documents which pointers are protected by RCU.</i></td></tr>
<tr><th id="329">329</th><td><i class="doc"> *</i></td></tr>
<tr><th id="330">330</th><td><i class="doc"> * atomic_rcu_read also includes a compiler barrier to ensure that</i></td></tr>
<tr><th id="331">331</th><td><i class="doc"> * value-speculative optimizations (e.g. VSS: Value Speculation</i></td></tr>
<tr><th id="332">332</th><td><i class="doc"> * Scheduling) does not perform the data read before the pointer read</i></td></tr>
<tr><th id="333">333</th><td><i class="doc"> * by speculating the value of the pointer.</i></td></tr>
<tr><th id="334">334</th><td><i class="doc"> *</i></td></tr>
<tr><th id="335">335</th><td><i class="doc"> * Should match atomic_rcu_set(), atomic_xchg(), atomic_cmpxchg().</i></td></tr>
<tr><th id="336">336</th><td><i class="doc"> */</i></td></tr>
<tr><th id="337">337</th><td><u>#define atomic_rcu_read(ptr)    ({                \</u></td></tr>
<tr><th id="338">338</th><td><u>    typeof(*ptr) _val = atomic_read(ptr);         \</u></td></tr>
<tr><th id="339">339</th><td><u>    smp_read_barrier_depends();                   \</u></td></tr>
<tr><th id="340">340</th><td><u>    _val;                                         \</u></td></tr>
<tr><th id="341">341</th><td><u>})</u></td></tr>
<tr><th id="342">342</th><td></td></tr>
<tr><th id="343">343</th><td><i class="doc">/**</i></td></tr>
<tr><th id="344">344</th><td><i class="doc"> * atomic_rcu_set - assigns (publicizes) a pointer to a new data structure</i></td></tr>
<tr><th id="345">345</th><td><i class="doc"> * meant to be read by RCU read-side critical sections.</i></td></tr>
<tr><th id="346">346</th><td><i class="doc"> *</i></td></tr>
<tr><th id="347">347</th><td><i class="doc"> * Documents which pointers will be dereferenced by RCU read-side critical</i></td></tr>
<tr><th id="348">348</th><td><i class="doc"> * sections and adds the required memory barriers on architectures requiring</i></td></tr>
<tr><th id="349">349</th><td><i class="doc"> * them. It also makes sure the compiler does not reorder code initializing the</i></td></tr>
<tr><th id="350">350</th><td><i class="doc"> * data structure before its publication.</i></td></tr>
<tr><th id="351">351</th><td><i class="doc"> *</i></td></tr>
<tr><th id="352">352</th><td><i class="doc"> * Should match atomic_rcu_read().</i></td></tr>
<tr><th id="353">353</th><td><i class="doc"> */</i></td></tr>
<tr><th id="354">354</th><td><u>#define atomic_rcu_set(ptr, i)  do {              \</u></td></tr>
<tr><th id="355">355</th><td><u>    smp_wmb();                                    \</u></td></tr>
<tr><th id="356">356</th><td><u>    atomic_set(ptr, i);                           \</u></td></tr>
<tr><th id="357">357</th><td><u>} while (0)</u></td></tr>
<tr><th id="358">358</th><td></td></tr>
<tr><th id="359">359</th><td><u>#define atomic_load_acquire(ptr)    ({      \</u></td></tr>
<tr><th id="360">360</th><td><u>    typeof(*ptr) _val = atomic_read(ptr);   \</u></td></tr>
<tr><th id="361">361</th><td><u>    smp_mb_acquire();                       \</u></td></tr>
<tr><th id="362">362</th><td><u>    _val;                                   \</u></td></tr>
<tr><th id="363">363</th><td><u>})</u></td></tr>
<tr><th id="364">364</th><td></td></tr>
<tr><th id="365">365</th><td><u>#define atomic_store_release(ptr, i)  do {  \</u></td></tr>
<tr><th id="366">366</th><td><u>    smp_mb_release();                       \</u></td></tr>
<tr><th id="367">367</th><td><u>    atomic_set(ptr, i);                     \</u></td></tr>
<tr><th id="368">368</th><td><u>} while (0)</u></td></tr>
<tr><th id="369">369</th><td></td></tr>
<tr><th id="370">370</th><td><u>#ifndef atomic_xchg</u></td></tr>
<tr><th id="371">371</th><td><u>#if defined(__clang__)</u></td></tr>
<tr><th id="372">372</th><td><u>#define atomic_xchg(ptr, i)    __sync_swap(ptr, i)</u></td></tr>
<tr><th id="373">373</th><td><u>#else</u></td></tr>
<tr><th id="374">374</th><td><i>/* __sync_lock_test_and_set() is documented to be an acquire barrier only.  */</i></td></tr>
<tr><th id="375">375</th><td><u>#define atomic_xchg(ptr, i)    (smp_mb(), __sync_lock_test_and_set(ptr, i))</u></td></tr>
<tr><th id="376">376</th><td><u>#endif</u></td></tr>
<tr><th id="377">377</th><td><u>#endif</u></td></tr>
<tr><th id="378">378</th><td><u>#define atomic_xchg__nocheck  atomic_xchg</u></td></tr>
<tr><th id="379">379</th><td></td></tr>
<tr><th id="380">380</th><td><i>/* Provide shorter names for GCC atomic builtins.  */</i></td></tr>
<tr><th id="381">381</th><td><u>#define atomic_fetch_inc(ptr)  __sync_fetch_and_add(ptr, 1)</u></td></tr>
<tr><th id="382">382</th><td><u>#define atomic_fetch_dec(ptr)  __sync_fetch_and_add(ptr, -1)</u></td></tr>
<tr><th id="383">383</th><td><u>#define atomic_fetch_add(ptr, n) __sync_fetch_and_add(ptr, n)</u></td></tr>
<tr><th id="384">384</th><td><u>#define atomic_fetch_sub(ptr, n) __sync_fetch_and_sub(ptr, n)</u></td></tr>
<tr><th id="385">385</th><td><u>#define atomic_fetch_and(ptr, n) __sync_fetch_and_and(ptr, n)</u></td></tr>
<tr><th id="386">386</th><td><u>#define atomic_fetch_or(ptr, n) __sync_fetch_and_or(ptr, n)</u></td></tr>
<tr><th id="387">387</th><td><u>#define atomic_fetch_xor(ptr, n) __sync_fetch_and_xor(ptr, n)</u></td></tr>
<tr><th id="388">388</th><td></td></tr>
<tr><th id="389">389</th><td><u>#define atomic_inc_fetch(ptr)  __sync_add_and_fetch(ptr, 1)</u></td></tr>
<tr><th id="390">390</th><td><u>#define atomic_dec_fetch(ptr)  __sync_add_and_fetch(ptr, -1)</u></td></tr>
<tr><th id="391">391</th><td><u>#define atomic_add_fetch(ptr, n) __sync_add_and_fetch(ptr, n)</u></td></tr>
<tr><th id="392">392</th><td><u>#define atomic_sub_fetch(ptr, n) __sync_sub_and_fetch(ptr, n)</u></td></tr>
<tr><th id="393">393</th><td><u>#define atomic_and_fetch(ptr, n) __sync_and_and_fetch(ptr, n)</u></td></tr>
<tr><th id="394">394</th><td><u>#define atomic_or_fetch(ptr, n) __sync_or_and_fetch(ptr, n)</u></td></tr>
<tr><th id="395">395</th><td><u>#define atomic_xor_fetch(ptr, n) __sync_xor_and_fetch(ptr, n)</u></td></tr>
<tr><th id="396">396</th><td></td></tr>
<tr><th id="397">397</th><td><u>#define atomic_cmpxchg(ptr, old, new) __sync_val_compare_and_swap(ptr, old, new)</u></td></tr>
<tr><th id="398">398</th><td><u>#define atomic_cmpxchg__nocheck(ptr, old, new)  atomic_cmpxchg(ptr, old, new)</u></td></tr>
<tr><th id="399">399</th><td></td></tr>
<tr><th id="400">400</th><td><i>/* And even shorter names that return void.  */</i></td></tr>
<tr><th id="401">401</th><td><u>#define atomic_inc(ptr)        ((void) __sync_fetch_and_add(ptr, 1))</u></td></tr>
<tr><th id="402">402</th><td><u>#define atomic_dec(ptr)        ((void) __sync_fetch_and_add(ptr, -1))</u></td></tr>
<tr><th id="403">403</th><td><u>#define atomic_add(ptr, n)     ((void) __sync_fetch_and_add(ptr, n))</u></td></tr>
<tr><th id="404">404</th><td><u>#define atomic_sub(ptr, n)     ((void) __sync_fetch_and_sub(ptr, n))</u></td></tr>
<tr><th id="405">405</th><td><u>#define atomic_and(ptr, n)     ((void) __sync_fetch_and_and(ptr, n))</u></td></tr>
<tr><th id="406">406</th><td><u>#define atomic_or(ptr, n)      ((void) __sync_fetch_and_or(ptr, n))</u></td></tr>
<tr><th id="407">407</th><td><u>#define atomic_xor(ptr, n)     ((void) __sync_fetch_and_xor(ptr, n))</u></td></tr>
<tr><th id="408">408</th><td></td></tr>
<tr><th id="409">409</th><td><u>#<span data-ppcond="63">endif</span> /* __ATOMIC_RELAXED */</u></td></tr>
<tr><th id="410">410</th><td></td></tr>
<tr><th id="411">411</th><td><u>#<span data-ppcond="411">ifndef</span> <span class="macro" data-ref="_M/smp_wmb">smp_wmb</span></u></td></tr>
<tr><th id="412">412</th><td><u>#define <dfn class="macro" id="_M/smp_wmb" data-ref="_M/smp_wmb">smp_wmb</dfn>()   smp_mb_release()</u></td></tr>
<tr><th id="413">413</th><td><u>#<span data-ppcond="411">endif</span></u></td></tr>
<tr><th id="414">414</th><td><u>#<span data-ppcond="414">ifndef</span> <span class="macro" data-ref="_M/smp_rmb">smp_rmb</span></u></td></tr>
<tr><th id="415">415</th><td><u>#define <dfn class="macro" id="_M/smp_rmb" data-ref="_M/smp_rmb">smp_rmb</dfn>()   smp_mb_acquire()</u></td></tr>
<tr><th id="416">416</th><td><u>#<span data-ppcond="414">endif</span></u></td></tr>
<tr><th id="417">417</th><td></td></tr>
<tr><th id="418">418</th><td><i>/* This is more efficient than a store plus a fence.  */</i></td></tr>
<tr><th id="419">419</th><td><u>#<span data-ppcond="419">if</span> !defined(<span class="macro" data-ref="_M/__SANITIZE_THREAD__">__SANITIZE_THREAD__</span>)</u></td></tr>
<tr><th id="420">420</th><td><u>#<span data-ppcond="420">if</span> defined(<span class="macro" data-ref="_M/__i386__">__i386__</span>) || defined(<span class="macro" data-ref="_M/__x86_64__">__x86_64__</span>) || defined(<span class="macro" data-ref="_M/__s390x__">__s390x__</span>)</u></td></tr>
<tr><th id="421">421</th><td><u>#define <dfn class="macro" id="_M/atomic_mb_set" data-ref="_M/atomic_mb_set">atomic_mb_set</dfn>(ptr, i)  ((void)atomic_xchg(ptr, i))</u></td></tr>
<tr><th id="422">422</th><td><u>#<span data-ppcond="420">endif</span></u></td></tr>
<tr><th id="423">423</th><td><u>#<span data-ppcond="419">endif</span></u></td></tr>
<tr><th id="424">424</th><td></td></tr>
<tr><th id="425">425</th><td><i>/* atomic_mb_read/set semantics map Java volatile variables. They are</i></td></tr>
<tr><th id="426">426</th><td><i> * less expensive on some platforms (notably POWER) than fully</i></td></tr>
<tr><th id="427">427</th><td><i> * sequentially consistent operations.</i></td></tr>
<tr><th id="428">428</th><td><i> *</i></td></tr>
<tr><th id="429">429</th><td><i> * As long as they are used as paired operations they are safe to</i></td></tr>
<tr><th id="430">430</th><td><i> * use. See docs/atomic.txt for more discussion.</i></td></tr>
<tr><th id="431">431</th><td><i> */</i></td></tr>
<tr><th id="432">432</th><td></td></tr>
<tr><th id="433">433</th><td><u>#<span data-ppcond="433">ifndef</span> <span class="macro" data-ref="_M/atomic_mb_read">atomic_mb_read</span></u></td></tr>
<tr><th id="434">434</th><td><u>#define <dfn class="macro" id="_M/atomic_mb_read" data-ref="_M/atomic_mb_read">atomic_mb_read</dfn>(ptr)                             \</u></td></tr>
<tr><th id="435">435</th><td><u>    atomic_load_acquire(ptr)</u></td></tr>
<tr><th id="436">436</th><td><u>#<span data-ppcond="433">endif</span></u></td></tr>
<tr><th id="437">437</th><td></td></tr>
<tr><th id="438">438</th><td><u>#<span data-ppcond="438">ifndef</span> <a class="macro" href="#421" data-ref="_M/atomic_mb_set">atomic_mb_set</a></u></td></tr>
<tr><th id="439">439</th><td><u>#define atomic_mb_set(ptr, i)  do {                     \</u></td></tr>
<tr><th id="440">440</th><td><u>    atomic_store_release(ptr, i);                       \</u></td></tr>
<tr><th id="441">441</th><td><u>    smp_mb();                                           \</u></td></tr>
<tr><th id="442">442</th><td><u>} while(0)</u></td></tr>
<tr><th id="443">443</th><td><u>#<span data-ppcond="438">endif</span></u></td></tr>
<tr><th id="444">444</th><td></td></tr>
<tr><th id="445">445</th><td><u>#<span data-ppcond="15">endif</span> /* QEMU_ATOMIC_H */</u></td></tr>
<tr><th id="446">446</th><td></td></tr>
</table><hr/><p id='footer'>
Generated while processing <a href='../../accel/accel.c.html'>codebrowser/accel/accel.c</a><br/>Generated on <em>2017-Aug-28</em> from project codebrowser revision <em>v2.10.0-rc0-7-g6be37cc</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
